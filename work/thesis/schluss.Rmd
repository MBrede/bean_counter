# Schlussbetrachtung

Ziel dieser Arbeit war es, ein Tool als Alternative zum Linienschnitt-Verfahren zur Auswertung elektronenmikroskopischer Aufnahmen von Kornstrukturen zu entwickeln. Dieses Vorhaben wurde in Kooperation mit dem Institut für Materialphysik der Georg-August-Universität Göttingen angestoßen, an dem bisher breit das Linienschnitt-Verfahren eingesetzt wird, das sehr aufwändig in der Durchführung und zusätzlich anfällig für Fehler ist. 
Damit das Tool sinnvoll eingesetzt werden kann, sollte es vergleichbare Kornverteilungen wie die händische Auswertung generieren und die gewohnten Vorverarbeitungsschritte abdecken. Außerdem ist der Einsatz erst dann sinnvoll, wenn die Nutzung des Tools zu einer deutlichen Beschleunigung der Auswertung führt. Eine weitere Voraussetzung für die Nachnutzbarkeit des Tools ist die einfache Erweiterbarkeit dessen.\newline
Um diese Anforderungen zu erfüllen, wurde ein Python-Tool entwickelt, das mit Hilfe von verschiedenen Superpixel-Algorithmen versucht, eine Auswertung von Kornbildern zu ermöglichen. Neben der Analyse der Bilder bietet das Tool die Möglichkeit, die Bilder vorzuverarbeiten und Ergebnisse per Maximum-Likelihood-Schätzung mit einer Verteilung der Wahl anzupassen, integriert also weitere Verarbeitungsschritte, die bisher separat geschahen, in ein Werkzeug. Das fertige Tool wurde zum einen in einem Praxistest, zum anderen mit Hilfe eines TPE in Hinblick auf die Lösungsgüte evaluiert. Während des Praxisstests musste dabei der Umfang der Analyse-Funktionen erweitert werden, um mit einer vorher nicht gesehenen Art an Bildern umzugehen, wobei die Erweiterbarkeit getestet und bewiesen werden konnte. 
Mit dem neuen Funktionsumfang konnte nach Finden der richtigen Parameter bei der ersten Aufnahme eine deutliche Beschleunigung der Auswertungszeit festgestellt werden. Bei dem Versuch, per TPE die Einstellungen der Vorverarbeitungs- und Superpixel-Algorithmen so zu optimieren, dass die Ergebnisse möglichst nah an den Linienschnitt-Ergebnissen lagen, konnte für zwei der drei implementierten Analyse-Methoden ein sehr ähnliches Ergebnis für alle Testbilder generiert werden.  \newline
Insgesamt kann der Lösungsansatz damit als Erfolg gewertet werden, die Anforderungen wurden zur Zufriedenheit der Mitarbeitenden der Uni Göttingen erfüllt. Insbesondere der Vergleich der Optimierungsverfahren SLIC und DBSCAN und die Notwendigkeit zur Anpassung des Tools im Praxistest verdeutlichen aber nochmal die Notwendigkeit der Erweiterbarkeit des Tools. Die Auswertungsverfahren waren unterschiedlich gut in der Auswertung unterschiedlicher Kornstrukturen - die Notwendigkeit anderer Verfahren bei neuen Bildern ist also naheliegend.
Das dritte implementierte Analyseverfahren konnte im Rahmen dieser Masterarbeit leider nicht erfolgreich zur Replikation der händisch gewonnenen Verteilungen genutzt werden. Wahrscheinlich hängt dieses Problem mit der Anzahl der Freiheitsgrade des Verfahrens und der damit wesentlich komplexeren Optimierungsaufgabe zusammen. Mit mehr Zeit und Ressourcen oder einem anderen Optimierungsansatz könnte dieses Problem aber gelöst werden. \newline
Der Einsatz eines anderen Optimierungsverfahrens könnte noch aus zwei anderen kritischen Perspektiven auf das verwendete Verfahren in Betracht gezogen werden. Zum einen schreiben die Autoren von BOHB selbst, dass ein Problem ohne sinnvolle Budget-Definition nicht die Vorteile des Verfahrens voll ausnutzt [@biedenkappAutoMLBOHBRobust2018], außerdem würden selbst bei optimalen Bedingungen andere Verfahren bessere Ergebnisse auf Benchmark-Problemen zeigen als BOHB [@eggenspergerHPOBenchCollectionReproducible2021]. Aus Gründen der Vergleichbarkeit der Ergebnisse wurde im Rahmen dieser Arbeit aber auf einen Wechsel des Optimierungsverfahrens verzichtet. 
Zum anderen könnte an dem Vorgehen, jedes Analyse-Verfahren für jedes Bild einzeln zu optimieren, kritisiert werden, dass Overfitting der wenigen Daten riskiert wird, die zur Verfügung stehen. Da das Tool aber nicht mit gefundenen Einstellungen generalisiert werden soll, sondern der intendierte Nutzen ist, die Parameter an ein gegebenes Bild anzupassen, ist das Overfitting im Sinne der Beeinträchtigung der prädiktiven Validität aber kein wirkliches Problem. Eine offene Frage ist aber, ob die durch die Optimierung gefundenen Kornverteilungen wirklich aus den ausgemessenen Körnern resultieren, oder - vor allem durch den setzbaren Parameter des maximalen Seitenverhältnisses - einfach eine Lösung gesucht wird, die dem Linienschnitt möglichst ähnliches Rauschen produziert. Dieser Punkt ist nicht von der Hand zu weisen, muss aber von Anwendern mit größerer Expertise beantwortet werden. Die Besprechung von Stichproben der Ergebnisse mit einem Mitarbeiter der Uni Göttingen wies nicht daraufhin, weitere Tests dazu bleiben aber nötig. \newline
Neben der hier benutzten Auswahl von klassischen Superpixelmethoden existiert noch ein großer Pool an anderen Möglichkeiten, wie zum Beispiel im Überblicksartikel von @stutzSuperpixelsEvaluationStateoftheart2018 beschrieben wird. Die Auswahl im Rahmen dieser Arbeit wurde auf Betrachtung der Lösungen von Stutz et al. gestützt, ist aber weder erschöpfend noch unbedingt die optimale, wenn auch die in Rücksprache mit Mitarbeitern der Uni Göttingen als am plausibelsten erscheinende. Weitere, systematische Tests der Anwendbarkeit einer Reihe anderer Verfahren scheint notwendig. Dies ist insbesondere so, da die erfolgreich getesteten Analyse-Verfahren SLIC und DBSCAN beide Cluster-Algorithmen einsetzen, um zu einer Lösung zu gelangen. Daneben werden bei @stutzSuperpixelsEvaluationStateoftheart2018 noch eine ganze andere Reihe von konzeptionellen Ansätzen beschrieben, wie zum Beispiel der auch ausgetestete Graph-Basierte Normalized Cuts Ansatz oder Ansätze der Energie-Optimierung wie zum Beispiel das Verfahren der Convexity Constrained Superpixels [@tasliConvexityConstrainedEfficient2015], für deren Implementierung und Test die Dauer dieser Masterarbeit nicht ausreichte.
Neben den klassischen Methoden zu Superpixel-Extraktion existieren auch Ansätze, Superpixel auf Basis neuronaler Netze zu erkennen und zu extrahieren, zum Beispiel nach dem bei @yangSuperpixelSegmentationFully2020 beschriebenen Verfahren. Dazu kommen noch die bereits beschriebenen Ansätze zur Extraktion von Kornstrukturen mit nicht Superpixel-basierten neuronalen Netzen [z.B. @latifDeepLearningBasedAutomaticMineral2022; @podorSEraMicSemiautomaticMethod2021]. Da von der Universität Göttingen aber nur 31 Bilder mit Kornverteilungen zur Verfügung gestellt werden konnten und der erste Schritt eine Entwicklung eines Tools zur Auswertungsunterstützung war, konnte eine Implementation auch dieser Verfahren im Rahmen dieser Arbeit nicht geleistet werden. \newline
Als weiterer Ansatzpunkt zur Verbesserung der Analyse bleibt die Beobachtung aus dem Praxistest: Die Analyse mit Tool ist zwar ab dem zweiten Bild wesentlich schneller als per Linienschnitt, die Analyse des ersten Bildes dauert aber länger. Hier könnte angesetzt werden, indem ein Modell trainiert wird, das auf Basis von vorliegenden Bildern und den Einstellungen, die zu optimalen Ergebnissen führen, eine Einstellungs-Empfehlung abgibt. Umzusetzen wäre diese Empfehlung durch eine Ergänzung der Kornbild-Klasse um eine Methode, die aus einer Liste an vom Modell vorhergesagten, numerischen Werten einen Batch-Stack und eine Analyse-Voreinstellung generiert. Damit die Einstellungs-Vorschläge nutzbar wären müsste letzteres dann als Aktualisierung an die GUI zurückgegeben werden. Diese Einstellungen könnten dann von der nutzenden Person als Ausgangspunkt zur Analyse eines vorliegenden Bildes genutzt werden und könnten dabei helfen das zeitaufwändige erste Suchen nach möglichen Einstellungen zu beschleunigen. Als möglicher Kandidat für ein solches Modell könnte eine Modifikation des von @zhuResidualAttentionSimple2021 vorgestellten Ansatzes zur Multi-Label Recognition mit Anpassung der Integration der Ergebnisse sein. Tests in diese Richtung waren auf Basis der zur Verfügung gestellten 31 Bilder aber leider nicht möglich, selbst bei Spiegeln anhand beider Achsen und damit einer Augmentierung des Datensatzes auf ein Volumen von 124 Bildern wäre nicht ausreichend. Mit mehr Bildern als Trainingsgrundlage könnte hier aber gut angesetzt werden, um den Nutzen des Tools schon beim ersten Bild zu erhöhen. \newline
Ein anderer Punkt, an dem zur Verbesserung der erstellten Lösung angesetzt werden kann, ist die Ergänzung der Methoden zur Vorverarbeitung der Bilder. Die Vorverarbeitung ist laut @princePartIVPreprocessing2012 nicht umsonst als mindestens so wichtig wie die Auswahl des Analysemodells beschrieben. In dieser Arbeit wurden aus Zeitgründen nur die Vorverarbeitungsschritte implementiert und getestet, die bisher auch für die Vorbereitung des Linienschnitt-Verfahrens am Institut für Materialphysik eingesetzt wurden, daneben existiert aber ein ganzer Blumenstrauß an Alternativen. Auf paperswithcode.com, einer Übersichtswebsite zur Performance von Machine Learning Algorithmen gemessen an einer Reihe von Benchmark-Datensätzen, finden sich alleine für die Aufgabe der Rauschentfernung zum Zeitpunkt des Schreibens 905 Ansätze [@PapersCodeDenoising]. Neben diesen ML-basierten Vorverarbeitungsmethoden existiert natürlich auch eine große Anzahl getesteter und etablierter klassischer Ansätze, auch hier würde eine erschöpfende Betrachtung aber auch den Rahmen sprengen.
Da aber das Tool so konstruiert ist, dass sowohl der Umfang der analytischen, als auch der Vorverarbeitungsmethoden ohne viel Aufwand erweiterbar ist, kann hier in Zukunft angesetzt werden, ohne dafür eine andere Lösung konstruieren zu müssen.